# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #5 выполнил:
- Иргашев Тимофей Александрович
- РИ210944
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * |  |
| Задание 2 | * |  |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

## Цель работы
Интеграция экономической системы в проект Unity и обучение ML-Agent

## Задание 1
### Изменить параметры yaml-агента и определить какие параметры и как влияют на обучение модели.
Для начала построим график с исходным yaml-агентом:

![l51](https://user-images.githubusercontent.com/103359810/209189462-3495c1d0-dc03-41c6-9090-d1ed2380fe13.PNG)

Теперь изменим некоторые параметры в файле Economic.yaml:
batch_size: 1024 => 512 (Количество опытов на каждой итерации)
buffer_size: 10240 => 15000 (Количество опыта, которое нужно набрать для обновления)
beta: 1.0e-2 => 5.0e-1 (Случайность действия. Повышает разнообразие и иследованность пространства обучения)
epsilon: 0.2 => 0.1 (Порог расхождений между старой и новой политиками при обновлении)
checkpoint_interval: 500.000 => 100.000 (Количество опыта, собираемое между каждым чекпоинтом.)

![l52](https://user-images.githubusercontent.com/103359810/209220481-146afae5-a509-4634-881b-b98225d3dd4a.PNG)

## Задание 2
### Описать результаты, выведенные в TensorBoard.
На графиках первого обучения видно, как Cumulative Reward (вознаграждение) стремительно растёт, так как нейрость обучается и вместе с этим растет награда. Episode Length (длительнось эпизода) и Policy Loss (реагирования на неудачи) рочти не меняются. Value Loss (потеря значимости обучения) падает. 

Второе обучение оказалось еще более успешным. За счёт увеличения опыта, необходимого для закрепления результата (buffer_size) и уменьшения порога расхождения между старой и новой логикой при обновлении (epsilon) нейросеть начала показывать стабильно высокий результат начиная с 10_000 шагов. В то же время потеря значимости обучения падает медленее.

Таким образом изменения конфигурации обучения нейросети положительно сказались на итоговом результате.



